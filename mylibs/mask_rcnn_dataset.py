# Author: Xinshuo
# Email: xinshuow@andrew.cmu.edu

import torch, numpy as np, torch.utils.data, random
from .general_utils import resize_image, compose_image_meta, generate_pyramid_anchors, mold_image, extract_bboxes, compute_overlaps_yx
from xinshuo_math import resize_mask, minimize_mask
from xinshuo_io import fileparts
from xinshuo_visualization import visualize_image, visualize_image_with_bbox

############################################################
#  Data Generator
############################################################
def load_image_gt(dataset, config, image_id, augment=False, use_mini_mask=False):
    """Load and return ground truth data for an image (image, mask, bounding boxes).

    augment: If true, apply random image augmentation. Currently, only horizontal flipping is offered.
    use_mini_mask: If False, returns full-size masks that are the same height and width as the original image. These can be big, for example
        1024x1024x100 (for 100 instances). Mini masks are smaller, typically, 224x224 and are generated by extracting the bounding box of the
        object and resizing it to MINI_MASK_SHAPE.

    Returns:
    image: [height, width, 3]
    shape: the original shape of the image before resizing and cropping.
    class_ids: [instance_count] Integer class IDs
    bbox: [instance_count, (y1, x1, y2, x2)]
    mask: [height, width, instance_count]. The height and width are those of the image unless use_mini_mask is True, in which case they are defined in MINI_MASK_SHAPE.
    """

    # Load image and mask
    image = dataset.load_image(image_id)                    # (height x width x 3)
    mask, class_ids = dataset.load_mask(image_id)           # (height x width x num_instances), (num_instances, )
    # print(mask.shape)                   
    # print(image.shape)
    # print(class_ids.shape)

    # TODO, check what happened here
    # assert len(mask.shape) == 3 and mask.shape[0] == image.shape[0] and mask.shape[1] == image.shape[1], 'the input mask shape is not correct'
    image, window, scale, padding = resize_image(image, min_dim=config.IMAGE_MIN_DIM, max_dim=config.IMAGE_MAX_DIM, padding=config.IMAGE_PADDING)
    mask = resize_mask(mask, scale, padding)

    # Random horizontal flips. TODO: add more data augmentation
    if augment:
        if random.randint(0, 1):
            image = np.fliplr(image)
            mask = np.fliplr(mask)

    # Bounding boxes. Note that some boxes might be all zeros if the corresponding mask got cropped out.
    bbox = extract_bboxes(mask)                           # (num_instances x 4)
    # TODO, make sure all boxes are bigger than 0, some masks might be small 

    # print(bbox.shape)
    # zxc

    # Active classes
    # Different datasets have different classes, so track the classes supported in the dataset of this image.
    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id]['source']]
    active_class_ids[source_class_ids] = 1

    if use_mini_mask: 
        # print(mask.shape)
        # print(config.MINI_MASK_SHAPE)
        # print(bbox.shape)

        # visualize_image(image, save_path='image.jpg')
        # for i in range(20):
            # visualize_image(mask[:, :, i], save_path='mask%03d.jpg' % i)

        mask = minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)          # Resize masks to smaller size to reduce memory usage
        # print(mask.shape)
        # zxc
    image_meta = compose_image_meta(image_id, image.shape, window, active_class_ids)          # Image meta data
    return image, image_meta, class_ids, bbox, mask

def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):
    """Given the anchors and GT boxes, compute overlaps and identify positive anchors and deltas to refine them to match their corresponding GT boxes.

    anchors: [num_anchors, (y1, x1, y2, x2)]
    gt_class_ids: [num_gt_boxes] Integer class IDs.
    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]

    Returns:
    rpn_match:  [N] (int32) matches between anchors and GT boxes. 1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_bbox_refine:   [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    N is max anchors per image
    """
    
    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

    # Handle COCO crowds
    # A crowd box in COCO is a bounding box around several instances. Exclude
    # them from training. A crowd box is given a negative class ID.
    crowd_ix = np.where(gt_class_ids < 0)[0]
    if crowd_ix.shape[0] > 0:
        # Filter out crowds from ground truth class IDs and boxes
        non_crowd_ix = np.where(gt_class_ids > 0)[0]
        crowd_boxes = gt_boxes[crowd_ix]
        gt_class_ids = gt_class_ids[non_crowd_ix]
        gt_boxes = gt_boxes[non_crowd_ix]
        
        # Compute overlaps with crowd boxes [anchors, crowds]
        crowd_overlaps = compute_overlaps_yx(anchors, crowd_boxes)
        crowd_iou_max = np.amax(crowd_overlaps, axis=1)
        no_crowd_bool = (crowd_iou_max < 0.001)
    else: no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)         # All anchors don't intersect a crowd
    overlaps = compute_overlaps_yx(anchors, gt_boxes)          # Compute overlaps [num_anchors, num_gt_boxes]

    # Match anchors to GT Boxes
    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive. If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
    # Neutral anchors are those that don't match the conditions above, and they don't influence the loss function.
    # However, don't keep any GT box unmatched (rare, but happens). Instead, match it to the closest anchor (even if its max IoU is < 0.3).
    
    # 1. Set negative anchors first. They get overwritten below if a GT box is matched to them. Skip boxes in crowd areas.
    anchor_iou_argmax = np.argmax(overlaps, axis=1)
    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]
    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1

    # 2. Set an anchor for each GT box (regardless of IoU value). TODO: If multiple anchors have the same IoU match all of them
    gt_iou_argmax = np.argmax(overlaps, axis=0)
    rpn_match[gt_iou_argmax] = 1
    rpn_match[anchor_iou_max >= 0.7] = 1            # 3. Set anchors with high overlap as positive.

    # Subsample to balance positive and negative anchors. Don't let positives be more than half the anchors
    ids = np.where(rpn_match == 1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
    if extra > 0:       # Reset the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0

    # Same for negative proposals
    ids = np.where(rpn_match == -1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE - np.sum(rpn_match == 1))
    if extra > 0:           # Rest the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0

    # For positive anchors, compute shift and scale needed to transform them to match the corresponding GT boxes.
    ids = np.where(rpn_match == 1)[0]
    ix = 0  # index into rpn_bbox
    # TODO: use box_refinment() rather than duplicating the code here
    for i, a in zip(ids, anchors[ids]):
        gt = gt_boxes[anchor_iou_argmax[i]]         # Closest gt box (it might have IoU < 0.7)

        # Convert coordinates to center plus width/height. GT Box
        gt_h = gt[2] - gt[0]
        gt_w = gt[3] - gt[1]
        gt_center_y = gt[0] + 0.5 * gt_h
        gt_center_x = gt[1] + 0.5 * gt_w
        
        # Anchor
        a_h = a[2] - a[0]
        a_w = a[3] - a[1]
        a_center_y = a[0] + 0.5 * a_h
        a_center_x = a[1] + 0.5 * a_w

        # Compute the bbox refinement that the RPN should predict.
        rpn_bbox[ix] = [(gt_center_y - a_center_y) / a_h, (gt_center_x - a_center_x) / a_w, np.log(gt_h / a_h), np.log(gt_w / a_w)]
        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV         # Normalize
        ix += 1

    return rpn_match, rpn_bbox

class Mask_RCNN_Dataset(torch.utils.data.Dataset):
    def __init__(self, dataset, config, augment=True):
        """A generator that returns images and corresponding target class ids,
            bounding box deltas, and masks.

            dataset: The Dataset object to pick data from
            config: The model config object
            shuffle: If True, shuffles the samples before every epoch
            augment: If True, applies image augmentation to images (currently only
                     horizontal flips are supported)

            Returns a Python generator. Upon calling next() on it, the
            generator returns two lists, inputs and outputs. The containtes
            of the lists differs depending on the received arguments:
            inputs list:
            - images: [batch, H, W, C]
            - image_metas: [batch, size of image meta]
            - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
            - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
            - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs
            - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
            - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
                        are those of the image unless use_mini_mask is True, in which
                        case they are defined in MINI_MASK_SHAPE.

            outputs list: Usually empty in regular training. But if detection_targets
                is True then the outputs list contains target class_ids, bbox deltas,
                and masks.
            """
        self.b = 0                      # batch item index
        self.image_index = -1
        self.image_ids = np.array(dataset.image_ids)     # a list of image id, length is number of data
        # print(self.image_ids)
        # zxc
        self.error_count = 0
        self.dataset = dataset
        self.config = config
        self.augment = augment
        self.skip = False
        self.anchors_ = generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, config.RPN_ANCHOR_RATIOS, config.BACKBONE_SHAPES, config.BACKBONE_STRIDES, config.RPN_ANCHOR_STRIDE)     # [anchor_count, (y1, x1, y2, x2)]

    def __getitem__(self, image_index):
        # Get GT bounding boxes and masks for image.
        image_path = self.dataset.image_info[image_index]['path']
        _, filename, _ = fileparts(image_path)

        # print(image_index)
        if image_index == 155: self.skip = False
        if self.skip: return [], [], [], [], [], [], [], image_index, filename
        
        image_id = self.image_ids[image_index]
        # print(image_index)
        # print(filename)
        image, image_metas, gt_class_ids, gt_boxes, gt_masks = load_image_gt(self.dataset, self.config, image_id, augment=self.augment, use_mini_mask=self.config.USE_MINI_MASK)

        # Skip images that have no instances. This can happen in cases
        # where we train on a subset of classes and the image doesn't
        # have any of the classes we care about.
        if not np.any(gt_class_ids > 0): return [], [], [], [], [], [], [], image_index, filename
        rpn_match, rpn_bbox = build_rpn_targets(image.shape, self.anchors_, gt_class_ids, gt_boxes, self.config)     # find the matches for every anchor and the deltas

        # If more instances than fits in the array, sub-sample from them.
        if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:
            ids = np.random.choice(np.arange(gt_boxes.shape[0]), self.config.MAX_GT_INSTANCES, replace=False)
            gt_class_ids = gt_class_ids[ids]
            gt_boxes = gt_boxes[ids]
            gt_masks = gt_masks[:, :, ids]

        rpn_match = rpn_match[:, np.newaxis]
        images = mold_image(image.astype(np.float32), self.config)      # Add to batch

        # Convert
        images = torch.from_numpy(images.transpose(2, 0, 1)).float()
        image_metas = torch.from_numpy(image_metas)
        rpn_match, rpn_bbox = torch.from_numpy(rpn_match), torch.from_numpy(rpn_bbox).float()
        gt_class_ids, gt_boxes, gt_masks = torch.from_numpy(gt_class_ids), torch.from_numpy(gt_boxes).float(), torch.from_numpy(gt_masks.astype(int).transpose(2, 0, 1)).float()
        return images, image_metas, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, image_index, filename

    def __len__(self):
        return self.image_ids.shape[0]